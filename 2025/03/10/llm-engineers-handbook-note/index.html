<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>《LLM Engineers Handbook》读书笔记 | Lost N Found</title><meta name="author" content="Guohao"><meta name="copyright" content="Guohao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="《LLM Engineer’s Handbook》本书的个人学习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="《LLM Engineers Handbook》读书笔记">
<meta property="og:url" content="https://lostnfound.top/2025/03/10/llm-engineers-handbook-note/index.html">
<meta property="og:site_name" content="Lost N Found">
<meta property="og:description" content="《LLM Engineer’s Handbook》本书的个人学习笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lostnfound.top/linear-gradient(45deg,%20#8EC3B0,%20#9ED5C5,%20#F8C4B4,%20#FF8787)">
<meta property="article:published_time" content="2025-03-10T05:04:09.000Z">
<meta property="article:modified_time" content="2025-03-11T07:53:20.425Z">
<meta property="article:author" content="Guohao">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="DeepLearning">
<meta property="article:tag" content="Engineering">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lostnfound.top/linear-gradient(45deg,%20#8EC3B0,%20#9ED5C5,%20#F8C4B4,%20#FF8787)"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="https://lostnfound.top/2025/03/10/llm-engineers-handbook-note/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":100,"languages":{"author":"作者: Guohao","link":"链接: ","source":"来源: Lost N Found","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '《LLM Engineers Handbook》读书笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-03-11 15:53:20'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="Lost N Found" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">42</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-battery-full"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-calendar-check"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-hashtag"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-user-circle"></i><span> 关于我</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://github.com/Dave0126"><i class="fa-fw fab fa-github"></i><span> Github</span></a></li><li><a class="site-page child" href="mailto:dave980126@outlook.com"><i class="fa-fw fas fa-envelope"></i><span> Mail</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background: linear-gradient(45deg, #8EC3B0, #9ED5C5, #F8C4B4, #FF8787)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Lost N Found</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-battery-full"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-calendar-check"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-hashtag"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-user-circle"></i><span> 关于我</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://github.com/Dave0126"><i class="fa-fw fab fa-github"></i><span> Github</span></a></li><li><a class="site-page child" href="mailto:dave980126@outlook.com"><i class="fa-fw fas fa-envelope"></i><span> Mail</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">《LLM Engineers Handbook》读书笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-10T05:04:09.000Z" title="发表于 2025-03-10 13:04:09">2025-03-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-11T07:53:20.425Z" title="更新于 2025-03-11 15:53:20">2025-03-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">13.3k</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>写在前面：</p>
<p>本文章是关于《LLM Engineer’s Handbook》的部分学习笔记。碍于本人学识有限，部分叙述难免存在纰漏，请读者注意甄别。</p>
<p>参考资料：</p>
<ul>
<li>《LLM Engineer’s Handbook》书中示例代码：[<code>Github</code>]( <a target="_blank" rel="noopener" href="https://github.com/PacktPublishing/">https://github.com/PacktPublishing/</a> LLM-Engineers-Handbook)</li>
</ul>
<hr>
<h1 id="零-前言"><a class="markdownIt-Anchor" href="#零-前言"></a> 零、前言</h1>
<p>大型语言模型（Large Language Model, LLM）工程领域迅速崛起，已成为人工智能和机器学习中的关键领域。随着 LLM 持续革新自然语言处理和生成技术，能够在实际场景中有效实施、优化和部署这些模型的专业人士需求呈指数级增长。LLM 工程涵盖从<u><em>数据准备</em></u>、<u><em>模型微调</em></u>到<u><em>推理优化</em></u>和<u><em>生产部署</em></u>等广泛学科，要求软件工程、机器学习专业知识和领域知识的独特融合。</p>
<p>机器学习运维（Machine Learning Operations, MLOps）在成功实施 LLM 于生产环境中起着至关重要的作用。MLOps 将 DevOps 的原则扩展至机器学习项目，专注于自动化和简化整个 ML 生命周期。对于 LLM 而言，由于其模型的复杂性和规模，MLOps 尤为重要。它解决了诸如管理大型数据集、处理模型版本控制、确保可重复性以及维持模型性能等挑战。通过融入 MLOps 实践，LLM 项目可以实现更高的效率、可靠性和可扩展性，最终促成更成功和有影响力的部署。</p>
<p>《LLM Engineer’s Handbook》是一本全面指南（以下简称“手册”），旨在将最佳实践应用于 LLM 工程领域。手册中涵盖数据工程、监督微调、模型评估、推理优化和检索增强生成（Retrieval-Augmented Generation, RAG）管道开发等主题。</p>
<p>为直观展示这些概念，手册中将开发一个名为 <code>LLM Twin</code> 的端到端项目，目标是模仿某人的写作风格和个性。该用例将展示如何构建一个最小可行产品以解决特定问题，运用 LLM 工程和 MLOps 的各个方面。我们读者将了解：</p>
<ul>
<li>如何为 LLM 收集和准备数据、针对特定任务微调模型、优化推理性能以及实施RAG管道；</li>
<li>如何评估LLM性能、使模型与人类偏好对齐；</li>
<li>部署基于LLM的应用程序。</li>
</ul>
<p>本笔记与手册结构保持一致，主要分为以下几个内容：</p>
<ul>
<li><strong>第1章 理解 <code>LLM Twin</code> 的概念和架构</strong>：介绍了贯穿全手册的 <code>LLM Twin</code> 项目，作为生产级 LLM 应用的端到端示例。定义了构建可扩展机器学习系统的 FTI 架构，并将其应用于 <code>LLM Twin</code> 的用例；</li>
<li><strong>第2章 工具和安装</strong>：介绍用于构建实际 LLM 应用的 Python、MLOps 和云工具，如编排器、实验跟踪器、提示监控和 LLM 评估工具。展示了如何在本地安装和使用这些工具以进行测试和开发；</li>
<li><strong>第3章 数据工程</strong>：介绍一个数据收集管道的实现，该管道从 <a target="_blank" rel="noopener" href="https://medium.com">Medium</a>、<a target="_blank" rel="noopener" href="https://github.com">GitHub</a> 和 <a target="_blank" rel="noopener" href="https://substack.com/home-i">Substack</a> 等多个网站抓取数据，并将原始数据存储在数据仓库中。强调了在实际机器学习应用中，从动态来源收集原始数据的重要性，而非依赖静态数据集；</li>
<li><strong>第4章 RAG 特征管道</strong>：介绍了检索增强生成（Retrieval-Augmented Generation, RAG）的基本概念，如 Embeddings、基础 RAG 框架、向量数据库，以及如何优化 RAG 应用。通过使用软件最佳实践，设计并实现了 LLM Twin 的 RAG 特征管道，以应用 RAG 理论；</li>
<li><strong>第5章 监督微调（Supervised Fine-Tuning, SFT）</strong>：探讨了使用**指令（instruction）-回答（answer）**对来优化预训练语言模型以执行特定任务的过程。涵盖了创建高质量数据集、实施全量微调（full fine-tuning）、LoRA 和 QLoRA 等微调技术，并提供了在自定义数据集上微调 <a href><code>Llama 3.1 8B</code></a> 模型的实践示范；</li>
<li><strong>第6章 带偏好对齐的微调</strong>：介绍了将语言模型与人类偏好对齐的技术，重点关注直接偏好优化（Direct Preference Optimization, DPO）。涵盖了创建自定义偏好数据集、实施 DPO，并提供了使用 <code>Unsloth</code> 库对 <code>TwinLlama-3.1-8B</code> 模型进行对齐的实践示范；</li>
<li><strong>第7章 评估LLM</strong>：详细描述了评估语言模型和 LLM 系统性能的各种方法。介绍通用和特定领域的评估，讨论流行的基准测试。本章包括对 <code>TwinLlama-3.1-8B</code> 模型的多标准实践评估；</li>
<li><strong>第8章 推理优化</strong>：涵盖了关键的优化策略，如推测解码、模型并行和权重量化。将讨论如何提高推理速度、降低延迟和最小化内存使用，介绍流行的推理引擎并比较其特性；</li>
<li><strong>第9章 RAG推理管道</strong>：通过从头开始实施自查询、重排序和过滤向量搜索等方法，探索高级 RAG 技术。涵盖了设计和实现 <code>LLM Twin</code> 的 RAG 推理管道，以及类似于 <code>LangChain</code> 等流行框架的自定义检索模块；</li>
<li><strong>第10章 推理管道部署</strong>：介绍了在线、异步和批量推理等机器学习部署策略，有助于将微调后的 <code>LLM Twin</code> 模型架构并部署到AWS SageMaker，并构建 FastAPI 微服务，将 RAG 推理管道作为 RESTful API 公开；</li>
<li><strong>第11章 MLOps和LLMOps</strong>：介绍了 LLMOps 的概念，从其在 DevOps 和 MLOps 中的根源开始。本章解释了如何将 <code>LLM Twin</code> 项目部署到云端，如将机器学习管道部署到 AWS，并展示了如何使用 Docker 容器化代码和构建 CI/CD/CT 管道。还在 <code>LLM Twin</code> 的推理管道上添加了提示监控层；</li>
<li><strong>附录 MLOps原则</strong>：涵盖了用于构建可扩展、可重复和健壮的机器学习应用的六项 MLOps 原则。</li>
</ul>
<h1 id="一-理解-llm-twin-的概念和架构"><a class="markdownIt-Anchor" href="#一-理解-llm-twin-的概念和架构"></a> 一、理解 <code>LLM Twin</code> 的概念和架构</h1>
<p>手册将教我们如何构建一个 <code>LLM Twin</code>，即一个通过将特定个人的写作风格、语气和个性融入 LLM 的 AI 角色。通过这个示例，我们将经历完整的 ML 生命周期，从数据收集到部署和监控。在实现 <code>LLM Twin</code> 的过程中学到的大多数概念都可以应用于其他基于 LLM 或 ML 的应用程序。</p>
<blockquote>
<p>The best way to learn about LLMs and production machine learning (ML) is to get your hands dirty and build systems.</p>
<p>学习LLM和生产级机器学习（ML）的最佳方式是亲自动手构建系统。</p>
</blockquote>
<p>在开始实施新产品时，从工程的角度来看，我们必须在开始构建之前经过三个规划步骤：</p>
<ul>
<li>首先，了解我们试图解决的问题以及我们想要构建的内容至关重要。在我们的案例中，<code>LLM Twin</code> 究竟是什么？为什么要构建它？这一步是我们必须思考并专注于**“WHY”**的地方。</li>
<li>其次，为了反映现实世界的场景，我们将设计一个具有最小功能的产品的初级版本。在这里，我们必须清楚地定义产品所具有的核心功能。这些选择是基于时间表、资源和团队知识做出的。这是我们在构思阶段和实际实施之间架起桥梁，并最终回答以下问题：“我们要构建什么（<strong>WHAT</strong>）？”。</li>
<li>最后，我们将进行系统设计步骤，列出用于构建 LLM 系统的核心架构和设计选型。前两个步主要与产品的设计相关，而最后一个是技术性的，专注于“<strong>HOW</strong>”。</li>
</ul>
<p>这三个步骤是在构建现实世界产品时自然发生的。虽然前两个不需要太多的 ML 知识，但是这对于产品的开发而言同样至关重要。简而言之，本章涵盖以下主题：</p>
<ul>
<li>理解 <code>LLM Twin</code> 概念</li>
<li>规划 <code>LLM Twin</code> 产品的最小可行产品（Minimum Viable Product, MVP）</li>
<li>使用特征/训练/推理管道构建 ML 系统</li>
<li>设计 <code>LLM Twin</code> 的系统架构</li>
</ul>
<h2 id="什么是-llm-twin"><a class="markdownIt-Anchor" href="#什么是-llm-twin"></a> 什么是 <code>LLM Twin</code></h2>
<p><code>LLM Twin</code> 是一个将个性化的写作风格、语气融入大型语言模型（LLM）中的 AI 角色。与在整个互联网数据上训练的通用 LLM 不同，<code>LLM Twin</code> 是在个人数据上进行微调的，将这些个人数据“<em>投射（projected）</em>”到大语言模型中。</p>
<blockquote>
<p>[!NOTE]</p>
<p>这里有意使用了“<em>投射（projected）</em>”一词。正如其他投射一样，在此过程中丢失大量信息，<strong>大模型只能反映出训练数据中信息</strong>。</p>
</blockquote>
<p>如果我们用鲁迅的文学作品微调 LLM，LLM 将会模仿鲁迅的写作风格，这也被称为风格迁移。我们将利用风格迁移策略，使 LLM 模仿我们个人的风格。</p>
<p>为了将 LLM 调整为特定的风格和语气，除了微调外，我们还将利用各种高级的检索增强生成（RAG）技术，以使用我们先前的 Embedding 来调节自回归过程。我们将在后续章节中详细探讨这些内容。</p>
<blockquote>
<p>[!TIP]</p>
<p><strong>什么是向量 Embedding ？</strong></p>
<p>在 LLM 的开发领域中，向量 Embedding 在获取文本信息的本质方面起着关键作用。向量 Embedding 的核心是指在数学空间中将单词、句子甚至整个文档表示为密集的低维向量的过程。与依赖于稀疏表示（如one-hot编码）的传统方法不同，向量 Embeddings封装了单词之间的语义关系，并使算法能够理解它们的上下文含义。</p>
<p>通过使用词 Embeddings、句子 Embeddings 或上下文 Embedding 等技术，向量 Embeddings 提供了文本数据的紧凑而有意义的表示。例如，单词 Embeddings 将单词映射到固定长度的向量，其中具有相似含义的单词在向量空间中的位置更接近。这允许高效的语义搜索、信息检索和语言理解任务。</p>
<p>向量 Embedding 的重要性在于它能够将原始文本转换为算法可以理解和推理的数字表示。这种转换过程不仅促进了各种自然语言处理（NLP）任务，而且还作为大型语言模型的基本构建块。向量 Embeddings 使这些模型能够利用嵌入在文本数据中的丰富语义信息，使它们能够生成更连贯和上下文更合适的响应。</p>
</blockquote>
<p>我们可以想象这样一个可以对 LLM 进行微调的场景：</p>
<ul>
<li>小红书、知乎等社交平台：使 LLM 仿照我们自己的风格来编写社交媒体内容；</li>
<li>学术论文和文章：微调 LLM 以撰写正式和学术性的内容；</li>
<li>代码：微调 LLM 使其以特定的代码规范来编写代码。</li>
</ul>
<p>所有上述场景都可以归结为一个核心策略：收集个人数据集（或其中的一部分），使用不同的算法将其输入到 LLM 中。最终，LLM 将反映所收集数据的语气和风格。</p>
<h3 id="为什么不用-qwen-这些通用大模型"><a class="markdownIt-Anchor" href="#为什么不用-qwen-这些通用大模型"></a> 为什么不用 <code>Qwen</code> 这些通用大模型？</h3>
<p><code>Qwen</code> 这些通用大模型非常通用、缺乏独特表达，且往往冗长。盲目使用通用大模型可能会导致以下问题：</p>
<ul>
<li><strong>幻觉导致的错误信息</strong>：需要手动检查生成内容是否存在幻觉，或使用第三方工具进行验证，是一项繁琐且低效的任务。</li>
<li><strong>繁琐的手动提示工程</strong>：需要手动编写提示词并注入外部信息，这个过程既耗时又麻烦。此外，由于无法完全控制提示词和输入数据，在不同会话中生成一致的答案也十分困难。虽然可以通过 API 和 <code>LangChain</code> 等工具部分解决此问题，但这需要一定的编程经验。</li>
</ul>
<p>如果想要高质量且真正有价值的内容时，我们可能会花比直接写作更多的时间去调试 AI 生成的文本。</p>
<p>由此可见，构建私人的大语言模型的关键点在于：</p>
<ul>
<li>我们收集哪些数据</li>
<li>如何预处理这些数据</li>
<li>如何将数据输入 LLM</li>
<li>如何链接多个提示以获得理想结果</li>
<li>如何评估生成的内容</li>
</ul>
<h3 id="规划产品的-mvp"><a class="markdownIt-Anchor" href="#规划产品的-mvp"></a> 规划产品的 MVP</h3>
<p>既然我们已经了解了什么是 LLM Twin 以及为什么要构建它，那么我们需要明确定义产品的功能。手册中重点关注 <code>LLM Twin</code> 的第一版，即<strong>最小可行产品（Minimum Viable Product, MVP）</strong>，以遵循大多数产品的自然发展周期。</p>
<h4 id="什么是-mvp"><a class="markdownIt-Anchor" href="#什么是-mvp"></a> 什么是 MVP?</h4>
<p>MVP 指的是产品的<strong>最小可行版本</strong>，即仅包含足够功能来吸引早期用户，并在开发的初始阶段验证产品概念的可行性。通常，MVP 的目标是以最小的投入<strong>从市场中收集反馈</strong>。</p>
<p>MVP 是一种产品策略，主要有以下优势：</p>
<ul>
<li><strong>加速产品上市（Accelerated time-to-market）</strong>：快速推出产品，以获得早期用户并建立市场影响力。</li>
<li><strong>验证产品理念（Idea validation）</strong>：在全面开发产品之前，通过真实用户进行测试，以验证产品是否符合需求。</li>
<li><strong>市场调研（Market research）</strong>：深入了解目标用户的偏好，收集有价值的市场反馈。</li>
<li><strong>降低风险（Risk minimization）</strong>：减少因产品市场表现不佳而浪费的时间和资源。</li>
</ul>
<p>在 MVP 中，必须严格<strong>遵循 “V”（Viable，可行性）</strong> 的原则，即产品必须是可行的。即使产品功能最小化，它也必须提供<strong>完整的用户体验</strong>，而不是半成品。MVP 需要是一个真正可用的产品，提供流畅的使用体验，让用户愿意持续使用，并随着产品的发展而发展。</p>
<h4 id="llm-twin-的-mvp-的核心功能"><a class="markdownIt-Anchor" href="#llm-twin-的-mvp-的核心功能"></a> <code>LLM Twin</code> 的 MVP 的核心功能</h4>
<p>为了保持简单性，我们的 LLM Twin MVP 将具备以下核心功能：</p>
<ol>
<li><strong>数据收集</strong>
<ul>
<li>从 <strong>小红书、知乎、微信 和 GitHub</strong> 账户收集用户的数据。</li>
</ul>
</li>
<li><strong>LLM 训练微调</strong>
<ul>
<li>使用<strong>开源 LLM</strong>，结合收集的数据进行微调（fine-tuning）。</li>
</ul>
</li>
<li><strong>RAG（检索增强生成）</strong>
<ul>
<li>将收集的数字数据存入<strong>向量数据库（vector database）</strong>，以支持 RAG 机制。</li>
</ul>
</li>
<li><strong>社交媒体内容生成</strong>（例如小红书文章）
<ul>
<li><strong>用户输入的提示（prompts）</strong></li>
<li><strong>RAG 检索</strong>，复用并引用用户过往内容</li>
<li><strong>新内容</strong>（如文章、论文等）作为 LLM 额外的知识输入</li>
</ul>
</li>
<li><strong>简单的 Web 界面</strong>，提供交互能力：
<ul>
<li><strong>配置社交媒体链接</strong>，并触发数据收集流程</li>
<li><strong>输入提示词（prompts）或外部资源链接</strong>，让 LLM Twin 生成内容</li>
</ul>
</li>
</ol>
<h4 id="mvp-的关键挑战"><a class="markdownIt-Anchor" href="#mvp-的关键挑战"></a> MVP 的关键挑战</h4>
<p>尽管上述 MVP 可能看起来功能不多，但我们必须确保<strong>系统具备以下特性</strong>：</p>
<ul>
<li><strong>成本可控</strong>（Cost-effective）：优化计算资源，避免不必要的开销。</li>
<li><strong>可扩展</strong>（Scalable）：随着用户增长，系统仍能稳定运行。</li>
<li><strong>模块化</strong>（Modular）：方便未来扩展和优化。</li>
</ul>
<p>至此，我们已经从<strong>用户和商业角度</strong>探讨了<code>LLM Twin</code> 的价值。<strong>最后一步</strong>，我们需要<strong>从工程实现的角度</strong>进行分析，并制定开发计划，明确<strong>如何在技术层面实现这个系统</strong>。</p>
<blockquote>
<p>[!NOTE]</p>
<p>从现在开始，重点将<strong>转向 <code>LLM Twin</code> 的具体实现</strong>。即使我们专注于上述<strong>核心功能</strong>，我们仍会基于<strong>最新的 LLM 研究成果</strong>，并结合<strong>最佳的软件工程与 MLOps 实践</strong>，构建<strong>一个成本可控、可扩展的 LLM 应用</strong>。</p>
</blockquote>
<h2 id="构建具有特征训练推理流水线的-ml-系统"><a class="markdownIt-Anchor" href="#构建具有特征训练推理流水线的-ml-系统"></a> 构建具有<code>特征/训练/推理流水线</code>的 ML 系统</h2>
<p>在深入探讨 <code>LLM Twin</code> 架构的具体细节之前，我们需要先理解其核心 ML 体系结构模式——<strong>特征/训练/推理（Feature/Training/Inference , FTI）架构</strong>。本节将概述 <strong>FTI 流水线</strong> 的设计，以及它如何帮助我们构建一个结构化的 ML 应用。</p>
<h3 id="ml-系统开发的挑战"><a class="markdownIt-Anchor" href="#ml-系统开发的挑战"></a> ML 系统开发的挑战</h3>
<p>构建生产级 ML 系统不仅仅是训练一个模型。从工程角度来看，训练模型通常是最简单的一步。然而，决定正确的架构和超参数，才是让模型真正发挥作用的挑战——这更像是一个研究问题，而不是纯粹的工程问题。</p>
<p>当前，我们关注的重点是<strong>如何设计一个可用于生产的架构</strong>。<strong>即使训练出了高准确率的模型，仅仅基于静态数据集训练它，距离真正的部署仍然很遥远。</strong> 我们需要考虑以下问题：</p>
<ul>
<li><strong>数据处理</strong>：如何**摄取（ingest）、清理（clean）和验证（validate）**新的数据？</li>
<li><strong>训练 vs 推理环境</strong>：训练和推理（Inference）环境是否需要<strong>分开部署</strong>？<strong>计算资源</strong> 如何分配？</li>
<li><strong>特征存储与计算</strong>：如何在<strong>正确的环境</strong>下<strong>计算并提供</strong>模型所需的特征？</li>
<li><strong>模型部署与服务</strong>：如何<strong>高效、低成本</strong>地提供推理服务？如何<strong>版本化、追踪并共享</strong>数据集和模型？</li>
<li><strong>监控与维护</strong>：如何<strong>监控</strong> ML 基础设施和模型的表现？<strong>模型如何扩展并持续更新</strong>？</li>
<li><strong>自动化</strong>：如何<strong>自动化</strong>模型的部署和训练流程？</li>
</ul>
<blockquote>
<p>[!TIP]</p>
<p>这些问题通常由 <strong>ML 或 MLOps 工程师</strong> 负责解决，而 <strong>研究团队或数据科学团队</strong> 主要关注<strong>模型训练</strong>本身。</p>
</blockquote>
<p>Google Cloud 团队提出的 <strong>成熟 ML &amp; MLOps 系统</strong> 需要包括的组件如下：</p>
<p><img src="/2025/03/10/llm-engineers-handbook-note/screenshot_2025-03-10_15.43.53.png" alt="screenshot_2025-03-10_15.43.53"></p>
<ul>
<li><strong>ML 代码</strong>（核心模型开发）</li>
<li><strong>数据收集</strong>（Data Collection）</li>
<li><strong>数据验证</strong>（Data Verification）</li>
<li><strong>测试与调试</strong>（Testing &amp; Debugging）</li>
<li><strong>资源管理</strong>（Resource Management）</li>
<li><strong>模型分析</strong>（Model Analysis）</li>
<li><strong>流程 &amp; 元数据管理</strong>（Process &amp; Metadata Management）</li>
<li><strong>服务基础设施</strong>（Serving Infrastructure）</li>
<li><strong>监控系统</strong>（Monitoring）</li>
</ul>
<p>可见，<strong>生产化 ML 模型远远不只是写好训练代码这么简单</strong>，它涉及多个环节和工程实践。</p>
<h3 id="如何构建一个统一的-ml-系统"><a class="markdownIt-Anchor" href="#如何构建一个统一的-ml-系统"></a> 如何构建一个统一的 ML 系统？</h3>
<p><strong>关键问题</strong>：如何将所有这些组件连接成<strong>一个统一的 ML 系统</strong>？我们需要设计一个标准化的架构，使 ML 系统的搭建更加高效、可复用和可扩展。</p>
<p>在传统软件工程中，很多应用可以拆分为 <strong>数据库（DB）、业务逻辑（Business Logic）和用户界面（UI）</strong> 三大部分。尽管每个部分的实现可能<strong>非常复杂</strong>，但在<strong>高层次的架构设计</strong>上，它们仍然可以归纳为这三大模块。</p>
<p>那么，ML 应用是否也能有类似的通用架构呢？</p>
<p>我们需要先回顾一些现有方案，看看它们为什么<strong>不适合构建可扩展的 ML 系统</strong>，然后再探索更优的解决方案。</p>
<h4 id="以往解决方案的问题"><a class="markdownIt-Anchor" href="#以往解决方案的问题"></a> 以往解决方案的问题</h4>
<p><img src="/2025/03/10/llm-engineers-handbook-note/screenshot_2025-03-10_15.47.41.png" alt="screenshot_2025-03-10_15.47.41"></p>
<p>在上图中，我们可以看到大多数 ML 应用程序中常见的架构。这种架构基于<strong>单体批处理（monolithic batch）<strong>模式，将</strong>特征创建（Create Features）</strong>、**模型训练（Train Model）<strong>和</strong>推理（Make Predictions）**紧密耦合在同一个组件中。</p>
<p>采用这种方法可以快速解决 ML 领域中的一个关键问题——<strong>训练-推理偏差（training-serving skew）</strong>。</p>
<ul>
<li><strong>训练-推理偏差</strong> 发生在<strong>训练时和推理时使用的特征计算方式不同</strong>，导致模型在生产环境中的表现不如预期。</li>
<li>在这种单体架构中，训练和推理阶段的特征是用<strong>相同的代码</strong>生成的，因此<strong>避免了训练-推理偏差</strong>。</li>
</ul>
<p><strong>单体批处理架构适用于小数据集</strong>，因为：</p>
<ul>
<li>训练、推理使用相同的特征计算代码，避免了训练-推理偏差</li>
<li>通过**批处理（batch mode）**定期运行流水线</li>
<li>预测结果通常被**第三方应用（如 dashboard）**消费</li>
</ul>
<p>然而，<strong>这种架构在面对更大规模的数据时，会引发许多问题</strong>：</p>
<ul>
<li><strong>特征无法复用</strong>（既不能在系统内部复用，也不能被其他系统使用）</li>
<li><strong>扩展性差</strong>，如果数据规模增加，必须重构代码以支持 <strong>PySpark</strong> 或 <strong>Ray</strong></li>
<li><strong>性能优化困难</strong>，如果想用 <strong>C++、Java 或 Rust</strong> 重写推理模块，会变得极为复杂</li>
<li><strong>团队协作受限</strong>，由于<strong>特征计算、训练和推理紧耦合在一起</strong>，难以拆分给不同的团队</li>
<li><strong>不支持流式计算</strong>，如果需要<strong>实时训练</strong>，无法切换到流式架构</li>
</ul>
<h5 id="单体架构在实时推理系统中的问题"><a class="markdownIt-Anchor" href="#单体架构在实时推理系统中的问题"></a> 单体架构在实时推理系统中的问题</h5>
<p><img src="/2025/03/10/llm-engineers-handbook-note/screenshot_2025-03-10_15.57.35.png" alt="screenshot_2025-03-10_15.57.35"></p>
<p>在上图中，我们可以看到类似的架构被应用于<strong>实时推理系统</strong>时会带来的额外问题。</p>
<p>在实时推理中，<strong>为了生成预测，我们必须通过客户端请求传输整个状态</strong>，以便计算特征并输入模型。例如，在电影推荐系统中，理想情况下，我们<strong>只需传递 userID</strong> 给模型，模型可以基于存储的用户数据计算推荐结果。但在单体架构中，我们必须传递<strong>整个用户状态</strong>，包括姓名、年龄、性别、观影历史等，使得客户端必须理解如何访问这些状态数据。</p>
<p><strong>这种方法极易出错</strong>，因为：</p>
<ul>
<li><strong>客户端和模型服务</strong> 强耦合，客户端必须知道如何查询和构造数据</li>
<li><strong>状态传输成本高</strong>，尤其在高并发情况下，传输大量状态信息会影响性能</li>
</ul>
<p>另一个例子是 <strong>LLM + RAG（检索增强生成）</strong> 的实现：</p>
<ul>
<li>在 <strong>RAG 模型</strong> 中，我们希望能基于<strong>外部知识库</strong>增强 LLM 的推理能力。</li>
<li>如果<strong>没有向量数据库（vector DB）</strong>，我们必须在每次查询时<strong>手动附带所有文档</strong>，否则模型无法参考这些外部知识。</li>
<li>这样就导致客户端需要<strong>手动查询和管理文档</strong>，这不仅<strong>不现实</strong>，而且是<strong>一种反模式（antipattern）</strong>。</li>
</ul>
<p>客户端不应负责查询和计算特征，而应交由服务端处理。</p>
<blockquote>
<p>我们将在<strong>第 8 章和第 9 章</strong>详细介绍 <strong>RAG</strong> 这一技术。</p>
</blockquote>
<p>综上所述，我们的<strong>核心问题是如何在不依赖客户端传递完整特征的情况下进行预测</strong>。</p>
<p>在<strong>另一端的极端案例</strong>，Google Cloud提供了一种**生产就绪（production-ready）**的、自动化流水线的 <a target="_blank" rel="noopener" href="https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning?hl=zh-cn">ML 架构</a>（见下图）。</p>
<p><img src="/2025/03/10/llm-engineers-handbook-note/screenshot_2025-03-10_16.10.48.png" alt="screenshot_2025-03-10_16.10.48"></p>
<p>这种架构确实<strong>能够解决生产环境中的 ML 部署问题</strong>，但它存在以下挑战：</p>
<ul>
<li><strong>复杂度高</strong>，不够直观，非 ML 生产专家很难理解</li>
<li><strong>上手难度大</strong>，如果你没有丰富的 ML 生产部署经验，可能会被架构的复杂性劝退</li>
<li><strong>不易渐进式扩展</strong>，难以理解如何从<strong>小型系统</strong>开始并随着需求增长逐步扩展</li>
</ul>
<p>在接下来的章节，我们将介绍 <strong>特征/训练/推理（Feature/Training/Inference , FTI）架构</strong>，它是一种直观的 ML 设计，能够有效<strong>解决前述的核心问题</strong>。</p>
<h3 id="特征训练推理fti-架构"><a class="markdownIt-Anchor" href="#特征训练推理fti-架构"></a> 特征/训练/推理（FTI） 架构</h3>
<blockquote>
<p>[!TIP]</p>
<p>想了解更多关于 FTI 模式的信息，可以参考*“From MLOps to ML Systems with Feature/Training/Inference Pipelines”* by Jim Dowling, CEO and co-founder of Hopsworks：<a target="_blank" rel="noopener" href="https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines">https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines</a></p>
</blockquote>
<p><strong>特征/训练/推理（Feature/Training/Inference , FTI）架构</strong>提出了一个清晰直接的思维框架，任何团队或个人都可以遵循它，来完成特征计算、模型训练以及推理管道的部署。该模式表明，任何机器学习系统都可以归结为三个管道：</p>
<ul>
<li>计算特征*（Feature）*</li>
<li>训练模型*（Training）*</li>
<li>进行推理*（Inference）*</li>
</ul>
<p>这种架构强大之处在于，我们可以清晰地定义每个管道的职责和接口。最终，系统只有三个核心模块，而不是像 Google Cloud 方案中展示的那种拥有二十个模块的复杂结构，这大大简化了操作和定义的难度。下图展示了特征、训练和推理管道架构。</p>
<p><img src="/2025/03/10/llm-engineers-handbook-note/screenshot_2025-03-10_16.11.00.png" alt="screenshot_2025-03-10_16.11.00"></p>
<p><strong>FTI 架构的核心特点</strong>：</p>
<ul>
<li><strong>每个管道都是独立的组件</strong>，可以<strong>在不同进程或硬件上运行</strong>。</li>
<li><strong>每个管道可以使用不同的技术实现</strong>，甚至可以由<strong>不同的团队开发和维护</strong>。</li>
<li><strong>可扩展性强</strong>，允许团队根据实际需求对不同管道<strong>独立扩展</strong>。</li>
<li><strong>提供清晰的思维导图</strong>，帮助团队<strong>高效组织 ML 系统架构</strong>。</li>
</ul>
<h4 id="特征管道feature-pipeline"><a class="markdownIt-Anchor" href="#特征管道feature-pipeline"></a> 特征管道（Feature Pipeline）</h4>
<p><strong>作用</strong>：<br>
特征管道的主要任务是<strong>从原始数据中提取特征</strong>，并生成用于<strong>模型训练或推理的特征和标签</strong>。但这些特征不会直接传递给模型，而是**存储在特征库（Feature Store）**中。</p>
<p><strong>主要职责</strong>：</p>
<ul>
<li><strong>存储、版本管理、追踪、共享</strong> 训练和推理所需的特征。</li>
<li><strong>保持特征的状态</strong>，确保训练和推理阶段使用的特征一致，从而<strong>避免训练-推理偏差（Training-Serving Skew）</strong>。</li>
<li><strong>让训练和推理管道轻松获取数据</strong>，保证系统的<strong>稳定性和可复现性</strong>。</li>
</ul>
<h4 id="训练管道training-pipeline"><a class="markdownIt-Anchor" href="#训练管道training-pipeline"></a> 训练管道（Training Pipeline）</h4>
<p><strong>作用</strong>：<br>
训练管道的任务是**从特征库中提取特征和标签，训练模型，并将训练好的模型存储在模型仓库（Model Registry）**中。</p>
<p><strong>主要职责</strong>：</p>
<ul>
<li>
<p>训练一个或多个模型，并<strong>存储、版本管理、追踪和共享</strong> 这些模型。</p>
</li>
<li>
<p><strong>模型仓库（Model Registry）</strong> 的角色类似于<strong>特征库</strong>，但重点是管理模型，而不是特征。</p>
</li>
<li>
<p>记录元数据（Metadata Store）</p>
<p>，包括：</p>
<ul>
<li>训练使用的<strong>特征、标签及其版本</strong>，确保模型的可追溯性。</li>
<li>确保团队可以随时知道<strong>模型的训练数据</strong>，方便调试和迭代。</li>
</ul>
</li>
</ul>
<h4 id="推理管道inference-pipeline"><a class="markdownIt-Anchor" href="#推理管道inference-pipeline"></a> 推理管道（Inference Pipeline）</h4>
<p><strong>作用</strong>：<br>
推理管道的任务是<strong>使用特征库中的特征数据和模型仓库中的训练模型进行推理</strong>，并生成最终的预测结果。</p>
<p><strong>主要职责</strong>：</p>
<ul>
<li>支持批量（Batch）或实时（Real-time）推理：
<ul>
<li><strong>批量模式</strong>：预测结果存入数据库（DB）。</li>
<li><strong>实时模式</strong>：预测结果直接返回给客户端。</li>
</ul>
</li>
<li><strong>版本管理</strong>：特征、标签、模型的版本都是可追踪的，这意味着可以<strong>灵活地升级或回滚模型部署</strong>。</li>
<li>动态调整模型与特征的连接关系：
<ul>
<li>例如，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">模</mi><mi mathvariant="normal">型</mi><msub><mi>M</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">模型M_{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord cjk_fallback">模</span><span class="mord cjk_fallback">型</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 可能使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">特</mi><mi mathvariant="normal">征</mi><msub><mi>f</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">特征f_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord cjk_fallback">特</span><span class="mord cjk_fallback">征</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">特</mi><mi mathvariant="normal">征</mi><msub><mi>f</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">特征f_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord cjk_fallback">特</span><span class="mord cjk_fallback">征</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">特</mi><mi mathvariant="normal">征</mi><msub><mi>f</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">特征f_3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord cjk_fallback">特</span><span class="mord cjk_fallback">征</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，而 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">模</mi><mi mathvariant="normal">型</mi><msub><mi>M</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">模型M_{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord cjk_fallback">模</span><span class="mord cjk_fallback">型</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 可能使用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">特</mi><mi mathvariant="normal">征</mi><msub><mi>f</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">特征f_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord cjk_fallback">特</span><span class="mord cjk_fallback">征</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">特</mi><mi mathvariant="normal">征</mi><msub><mi>f</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">特征f_3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord cjk_fallback">特</span><span class="mord cjk_fallback">征</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">特</mi><mi mathvariant="normal">征</mi><msub><mi>f</mi><mn>4</mn></msub></mrow><annotation encoding="application/x-tex">特征f_4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord cjk_fallback">特</span><span class="mord cjk_fallback">征</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
<li>通过版本管理，我们可以<strong>快速切换或调整特征与模型的映射关系</strong>。</li>
</ul>
</li>
</ul>
<h2 id="设计-llm-twin-的系统结构"><a class="markdownIt-Anchor" href="#设计-llm-twin-的系统结构"></a> 设计 <code>LLM Twin</code> 的系统结构</h2>
<h3 id="需求分析"><a class="markdownIt-Anchor" href="#需求分析"></a> 需求分析</h3>
<p>系统需要具备以下数据处理能力：</p>
<ul>
<li><strong>数据采集</strong>：自动化并<strong>定期</strong> 从 <strong>小红书、知乎 和 GitHub</strong>（如果可行） 抓取数据。</li>
<li><strong>数据存储与标准化</strong>：统一格式化爬取的数据，并存入<strong>数据仓库（Data Warehouse）</strong>。</li>
<li><strong>数据清理</strong>：处理 <strong>噪声数据、重复数据和异常数据</strong>，确保数据质量。</li>
<li><strong>指令数据集（Instruction Dataset）构建</strong>：生成 <strong>用于微调 LLM 的训练数据集</strong>。</li>
<li><strong>数据向量化与存储</strong>：<strong>切分（Chunking）和嵌入（Embedding）</strong> 清理后的数据。<strong>存储向量化数据到向量数据库（Vector DB）</strong>，以支持 <strong>RAG</strong>。</li>
</ul>
<h4 id="训练training需求"><a class="markdownIt-Anchor" href="#训练training需求"></a> 训练（Training）需求</h4>
<ul>
<li>
<p><strong>支持多种 LLM 微调</strong>：</p>
<ul>
<li>支持 <strong>不同规模的 LLM（7B、14B、30B、70B 参数）</strong>；</li>
<li>能够基于 <strong>不同规模的指令数据集</strong> 进行微调。</li>
<li>支持不同 LLM 模型类型（如 <strong>Mistral、Llama、GPT</strong> 之间切换）。</li>
</ul>
</li>
<li>
<p><strong>实验管理</strong>：<strong>跟踪和比较</strong> 训练实验结果，优化模型效果。</p>
</li>
<li>
<p><strong>自动化训练</strong>：<strong>自动启动</strong> 训练任务，当新的 <strong>指令数据集可用</strong> 时触发训练流程。<strong>在部署前</strong> 测试潜在的 <strong>生产 LLM 候选模型</strong>，确保高质量推理能力。</p>
</li>
</ul>
<h4 id="推理inference需求"><a class="markdownIt-Anchor" href="#推理inference需求"></a> 推理（Inference）需求</h4>
<ul>
<li>
<p><strong>REST API 接口</strong>：提供 <strong>REST API</strong>，允许客户端与 <strong>LLM Twin</strong> 交互。</p>
</li>
<li>
<p><strong>实时访问向量数据库（Vector DB）</strong>：<strong>支持 RAG</strong>，确保推理时可以实时检索相关知识数据。</p>
</li>
<li>
<p><strong>多模型推理能力</strong>：<strong>支持不同规模的 LLM 进行推理</strong>，适应不同业务场景。</p>
</li>
<li>
<p><strong>自动扩展（Auto-Scaling）</strong>：<strong>根据用户请求负载自动扩展推理服务</strong>，优化计算资源分配。</p>
</li>
<li>
<p><strong>自动化部署</strong>：<strong>通过评估机制</strong>，自动部署 <strong>通过测试的 LLM</strong> 版本，减少手动干预。</p>
</li>
</ul>
<h4 id="llmops-需求"><a class="markdownIt-Anchor" href="#llmops-需求"></a> LLMOps 需求</h4>
<ul>
<li>
<p><strong>指令数据集管理</strong>：<strong>支持版本控制</strong>、<strong>数据 lineage 追踪</strong> 和 <strong>数据集复用</strong>，提高数据可管理性。</p>
</li>
<li>
<p><strong>模型管理</strong>：<strong>支持模型版本控制</strong>、<strong>模型 lineage 追踪</strong> 和 <strong>模型复用</strong>，便于模型管理和回溯。</p>
</li>
<li>
<p><strong>实验追踪</strong>：<strong>记录所有实验配置、结果和性能指标</strong>，确保可重复性和优化。</p>
</li>
<li>
<p><strong>CI/CD + 持续训练（Continuous Training）</strong>：<strong>支持 CT/CI/CD</strong>，即持续训练（CT）、持续集成（CI）和持续部署（CD）。</p>
</li>
<li>
<p><strong>提示词和系统监控</strong>：监控**提示词（Prompt）**的表现，防止偏差。<strong>系统监控</strong>，确保 LLM 服务稳定运行。</p>
</li>
</ul>
<h3 id="如何使用-fti-管道设计-llm-twin-架构"><a class="markdownIt-Anchor" href="#如何使用-fti-管道设计-llm-twin-架构"></a> 如何使用 FTI 管道设计 LLM Twin 架构</h3>
<p>我们将系统拆分为<strong>四个核心组件</strong>。除了 FTI 的三大核心管道（<strong>特征、训练、推理</strong>）外，我们还必须实现<strong>数据管道</strong>。</p>
<p>但在我们的场景下，我们的目标是<strong>在小团队中构建一个 MVP（最小可行产品）</strong>，因此：我们必须<strong>同时实现数据收集和 FTI 管道</strong>。这种 <strong>端到端开发模式</strong> 在初创公司中非常常见，因为资源有限，无法分配独立团队。工程师需要 <strong>跨多个角色</strong>，视项目进度调整工作内容。即使未来团队扩展，<strong>理解端到端 ML 系统的架构仍然至关重要</strong>，有助于协同开发与优化。</p>
<p><img src="/2025/03/10/llm-engineers-handbook-note/screenshot_2025-03-11_10.13.07.png" alt="screenshot_2025-03-11_10.13.07"></p>
<h4 id="数据收集管道data-collection-pipeline"><a class="markdownIt-Anchor" href="#数据收集管道data-collection-pipeline"></a> 数据收集管道（Data Collection Pipeline）</h4>
<p>数据收集管道的任务是<strong>爬取你的个人数据</strong>，包括：<strong>小红书</strong>、<strong>知乎</strong>（帖子、文章），<strong>GitHub</strong>（代码）</p>
<p>在架构上，该管道遵循<strong>ETL（提取-加载-转换）模式</strong>，即：</p>
<ul>
<li><strong>提取（Extract）</strong>：从社交媒体平台爬取数据；</li>
<li><strong>转换（Transform）</strong>：对数据进行标准化处理；</li>
<li><strong>加载（Load）</strong>：将数据存入<strong>数据仓库（NoSQL 数据库）</strong>。</li>
</ul>
<blockquote>
<p>[!NOTE]</p>
<p><strong>为什么使用 NoSQL 作为数据仓库？</strong></p>
<p>由于我们处理的是<strong>文本数据</strong>，它<strong>天然是非结构化的</strong>，因此 <strong>NoSQL 数据库（如 MongoDB）是最佳选择</strong>。尽管 <strong>MongoDB 不是传统的关系型数据库</strong>，但在我们的架构中，它将充当<strong>数据库的角色</strong>，因为：</p>
<ul>
<li>它存储了<strong>标准化的原始数据</strong>，这些数据由 <strong>ETL 管道收集</strong>并可以<strong>直接用于 ML 训练</strong>。</li>
<li>它适合<strong>灵活存储和查询非结构化文本数据</strong>，便于下游管道访问和处理。</li>
</ul>
</blockquote>
<p>为了更好地处理数据，我们将爬取的数据分为<strong>三类</strong>：</p>
<ul>
<li><strong>文章（Articles）</strong> → 知乎</li>
<li><strong>帖子（Posts）</strong> → 小红书</li>
<li><strong>代码（Code）</strong> → GitHub</li>
</ul>
<p>我们希望<strong>抽象化数据来源</strong>，即：在 LLM 训练或推理时，<strong>数据的来源不重要</strong>；但为了<strong>溯源和引用</strong>，我们会将**原始 URL 作为元数据（metadata）**存储。</p>
<p>从 <strong>数据处理、微调训练（Fine-tuning）和 RAG（检索增强生成）</strong> 的角度来看，<strong>知道数据类别比知道来源更重要</strong>。</p>
<ul>
<li>例如，不同数据类型的切分（chunking）策略会有所不同：
<ul>
<li><strong>帖子（Post）</strong> 和 <strong>文章（Article）</strong> 的分割方式不同。</li>
<li><strong>代码（Code）</strong> 需要额外的解析和上下文理解。</li>
</ul>
</li>
</ul>
<p>按<strong>类别</strong>（category）而非来源（source）组织数据，能提高系统的扩展性：</p>
<ul>
<li>例如： 小红书的数据可以直接纳入<strong>Posts 类别</strong>，无须改动处理逻辑；而<strong>GitLab</strong> 的代码数据可以无缝集成到 <strong>Code 类别</strong>。</li>
</ul>
<h4 id="特征管道feature-pipeline-2"><a class="markdownIt-Anchor" href="#特征管道feature-pipeline-2"></a> 特征管道（Feature Pipeline）</h4>
<p>特征管道的核心作用是<strong>从数据仓库获取原始数据</strong>（文章、帖子、代码），<strong>进行处理后存入特征存储（Feature Store）</strong>。<strong>FTI 设计模式的核心特点在此体现</strong>，但 LLM Twin 的特征管道有一些<strong>自定义特性</strong>：</p>
<ul>
<li>针对三种数据类型（文章、帖子、代码）分别进行不同的处理</li>
<li>包含三大核心步骤（<strong>清洗、切分、嵌入</strong>），用于微调 LLM 和 RAG（检索增强生成）</li>
<li>创建两种数据快照：
<ul>
<li><strong>清洗后数据</strong>（用于 LLM 微调）</li>
<li><strong>嵌入（Embedding）后数据</strong>（用于 RAG）：使用逻辑特征存储（Logical Feature Store），而非传统的专用特征存储。</li>
</ul>
</li>
</ul>
<p><strong>逻辑特征存储：向量数据库（Vector DB）</strong></p>
<p>在 RAG 系统中，向量数据库（Vector DB）是关键基础设施。向量数据库本质上是 <strong>NoSQL 数据库</strong>，可以按 <strong>ID 和集合名称（collection name）</strong> 访问数据点（datapoints）。我们可以<strong>查询 Vector DB 中的新数据</strong>，而不需要<strong>执行向量搜索（Vector Search）</strong>。处理后的数据会被封装为<strong>版本化、可追踪、可共享的处理后的数据（artifact）</strong>（关于 artifact 的细节将在第 2 章讨论）。</p>
<blockquote>
<p>[!TIP]</p>
<p>What is an <strong>artifact</strong> in computer science?</p>
<p>To put it simply, an artifact is a by-product of software development. It’s anything that is created so a piece of software can be developed. This might include things like data models, diagrams, setup scripts — the list goes on.</p>
<p>（简单来说，Artifact 指的是一种软件开发的副产品。它指的是任何创建出来用以开发一套软件的一类东西，这其中也许包含了数据模型，图表，启动脚本等等。）</p>
</blockquote>
<p>系统的其余部分将如何访问逻辑特征存储？**训练管道（Training Pipeline）**将指令数据集（Instruction Datasets）视为 artifact。<strong>推理管道（Inference Pipeline）</strong> 通过 <strong>向量搜索（Vector Search）</strong> 查询 <strong>Vector DB</strong> 以获取额外上下文信息。</p>
<p>对于我们的用例 <code>LLM Twin</code>，这已经足够了，因为：指令数据集（artifact）非常适合用于离线训练，而向量数据库是为在线访问而构建的，这是我们进行推理所需要的。</p>
<p>不过，在后续章节中，我们将解释如何**清理（cleaned）、分块（chunked）和嵌入（embedded）**这三个数据类别（文章、帖子和代码）。</p>
<h4 id="训练管道training-pipeline-2"><a class="markdownIt-Anchor" href="#训练管道training-pipeline-2"></a> 训练管道（Training Pipeline）</h4>
<p>训练管道的核心职责是 <strong>从特征存储（Feature Store）获取指令数据集（Instruct Dataset），微调 LLM，并将训练好的 LLM 权重存入模型注册表（Model Registry）</strong>。更具体地说：</p>
<ol>
<li>触发训练：当逻辑特征存储中有新的指令数据集（artifact）可用时，我们将触发训练管道，使用工件并微调 LLM。</li>
<li>超参数优化：
<ul>
<li>在初始阶段，数据科学团队负责这一步。他们通过自动或手动进行多次实验，以找到最适合的模型和超参数。</li>
<li>为了比较和挑选最佳超参数集，记录所有有价值的东西，并在实验之间进行比较。</li>
<li>最终，他们将挑选最佳超参数和微调后的 LLM，并将其作为 LLM 生产候选方案提出。然后将提议的 LLM 存储在模型注册表中。实验阶段结束后，我们存储并重复使用找到的最佳超参数。</li>
</ul>
</li>
<li>如今，我们可以完全自动化训练过程，即持续训练（Continuous Training, CT）。我们的模块化设计使我们能够快速利用 ML 编排器来安排和触发不同的系统部分。例如，我们可以安排数据收集管道每周抓取数据。然后，当数据仓库中有新数据可用时，我们可以触发特征管道，当有新的指令数据集可用时，我们可以触发训练管道。</li>
</ol>
<p>在将新模型推向生产之前，根据更严格的测试集对其进行评估至关重要，以确保最新候选模型比当前生产模型更好。如果此步骤通过，模型最终将被标记为已接受并部署到生产推理管道。即使在完全自动化的 ML 系统中，也建议在接受新的生产模型之前进行手动步骤。因此，在此阶段，专家会查看测试组件生成的报告。如果一切看起来都很好，它就会批准该模型，自动化可以继续。</p>
<p><strong>关键技术问题</strong></p>
<ul>
<li>如何设计一个 LLM 无关的训练管道？</li>
<li>应该使用哪些微调技术？</li>
<li>如何扩展微调算法，使其适用于不同规模的 LLM 和数据集？</li>
<li>如何从多个实验中选取最优 LLM 作为生产候选？</li>
<li>如何测试 LLM，以决定是否推送到生产环境？</li>
</ul>
<p>在后面的章节中，我们将一一介绍解决方案。</p>
<h4 id="推理管道inference-pipeline-2"><a class="markdownIt-Anchor" href="#推理管道inference-pipeline-2"></a> 推理管道（Inference Pipeline）</h4>
<p>推理管道是 <strong>LLM 系统的最后一个核心组件</strong>，负责处理用户查询并返回答案。它连接 <strong>模型注册表（Model Registry）</strong> 和 <strong>逻辑特征存储（Logical Feature Store）</strong>，用于加载微调后的 LLM 并执行 RAG（检索增强生成，Retrieval-Augmented Generation）。</p>
<p><strong>推理流程</strong></p>
<ol>
<li><strong>加载模型</strong>：从模型注册表加载已微调的 LLM；从逻辑特征存储访问向量数据库（Vector DB），用于 RAG 查询。</li>
<li><strong>接收客户端请求</strong>：通过 <strong>REST API</strong> 接收用户查询；解析查询并生成 RAG 任务。</li>
<li><strong>执行 RAG 以增强 LLM 生成能力</strong>：使用 <strong>向量数据库进行检索（Vector Search）</strong>，找到相关外部信息；结合 LLM 进行答案生成，返回最终响应。</li>
<li><strong>监控与分析</strong>：所有用户查询、RAG 处理的增强提示（Enriched Prompts）和生成结果，都会发送至<strong>提示监控系统（Prompt Monitoring System）</strong>。监控系统 <strong>分析、调试</strong> 模型输出，优化 LLM 行为。可根据 <strong>特定需求</strong> 触发警报，执行手动或自动调整。</li>
</ol>
<h3 id="fti-设计与-llm-twin-架构的最终思考"><a class="markdownIt-Anchor" href="#fti-设计与-llm-twin-架构的最终思考"></a> FTI 设计与 <code>LLM Twin</code> 架构的最终思考</h3>
<p>FTI（Feature-Training-Inference）模式 <strong>并不需要严格遵循</strong>，它的核心作用是<strong>帮助清晰地设计 ML（机器学习）系统</strong>。例如，我们的系统并没有使用传统的特征存储（Feature Store），而是选择了 <strong>基于向量数据库（Vector DB）和指令数据集（Artifacts）</strong> 的<strong>逻辑特征存储（Logical Feature Store）</strong>，因为这样更简单且成本更低。重点是提供一个<strong>可版本化（Versioned）且可复用（Reusable）</strong> 的训练数据集，而不是形式上的标准化存储。</p>
<p><strong>计算资源需求及可扩展性</strong></p>
<ul>
<li><strong>数据收集管道 &amp; 特征管道</strong>：
<ul>
<li>主要依赖 <strong>CPU 计算</strong>，对计算资源需求较低。</li>
<li>基于 <strong>CPU &amp; RAM 负载</strong> <strong>水平扩展（Horizontal Scaling）</strong></li>
</ul>
</li>
<li><strong>训练管道</strong>：
<ul>
<li>需要 <strong>强大的 GPU 计算能力</strong> 来加载和微调 LLM。</li>
<li>通过 <strong>增加 GPU 资源</strong> <strong>垂直扩展（Vertical Scaling）</strong></li>
</ul>
</li>
<li><strong>推理管道</strong>：
<ul>
<li>计算需求介于数据管道和训练管道之间，需要 <strong>较强计算能力</strong> 以确保低延迟。</li>
<li>基于 <strong>客户端请求数量</strong> <strong>水平扩展（Horizontal Scaling）</strong>。</li>
</ul>
</li>
</ul>
<p>推理管道直接面向用户，因此<strong>必须严格测试</strong>，确保<strong>延迟符合预期</strong>，从而提供良好的用户体验。FTI 设计使得计算资源的分配变得灵活，我们可以为不同的组件选择最合适的计算架构。</p>
<h2 id="引用"><a class="markdownIt-Anchor" href="#引用"></a> 引用</h2>
<ol>
<li>Dowling, J. (2024a, July 11). <em>From MLOps to ML Systems with Feature/Training/Inference Pipelines</em>. <em>Hopsworks</em>. <a target="_blank" rel="noopener" href="https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines">https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines</a></li>
<li>Dowling, J. (2024b, August 5). <em>Modularity and Composability for AI Systems with AI Pipelines and Shared Storage</em>. <em>Hopsworks</em>. <a target="_blank" rel="noopener" href="https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage">https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage</a></li>
<li>Joseph, M. (2024, August 23). <em>The Taxonomy for Data Transformations in AI Systems</em>. <em>Hop- sworks</em>. <a target="_blank" rel="noopener" href="https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems">https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems</a></li>
<li><em>MLOps: Continuous delivery and automation pipelines in machine learning</em>. (2024, August 28). Google Cloud. <a target="_blank" rel="noopener" href="https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning">https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning</a></li>
<li>Qwak. (2024a, June 2). <em>CI/CD for Machine Learning in 2024: Best Practices to build, test, and Deploy</em> | Infer. <em>Medium</em>. <a target="_blank" rel="noopener" href="https://medium.com/infer-qwak/ci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2">https://medium.com/infer-qwak/ci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2</a></li>
<li>Qwak. (2024b, July 23). <em>5 Best Open Source Tools to build End-to-End MLOPs Pipeline</em> in 2024. <em>Medium</em>. <a target="_blank" rel="noopener" href="https://medium.com/infer-qwak/building-an-end-to-end-mlops-pipeline-with-open-source-tools-d8bacbf4184f">https://medium.com/infer-qwak/building-an-end-to-end-mlops-pipeline-with-open-source-tools-d8bacbf4184f</a></li>
<li>Salama, K., Kazmierczak, J., &amp; Schut, D. (2021). <em>Practitioners guide to MLOPs: A framework for continuous delivery and automation of machine learning</em> (1st ed.) [PDF]. Google Cloud. <a target="_blank" rel="noopener" href="https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf">https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf</a></li>
</ol>
<h1 id="二-工具链与安装"><a class="markdownIt-Anchor" href="#二-工具链与安装"></a> 二、工具链与安装</h1>
<blockquote>
<p>[!CAUTION]</p>
<p>在手册中的对应章节介绍了使用的所有<strong>核心工具</strong>，特别是 <strong>LLM Twin 项目</strong> 的实现和部署所需的工具。因为个人偏好不同、对于工具链的选择亦有不同，所以在这里不做详细介绍，只会简要介绍手册作者推荐的工具链。</p>
<p>如果你<strong>熟悉这些工具</strong>，可以<strong>直接跳过</strong>本章。</p>
</blockquote>
<p>在本章，我们不会深入讲解 LLM、RAG、MLOps 或 LLMOps 的概念，而是快速概览我们的技术栈和前置要求，避免后续章节重复讲解工具安装和选择原因。从 <strong>第 3 章开始</strong>，我们将正式进入 <code>LLM Twin</code> 的应用场景，并<strong>实现一个 ETL 数据收集流程</strong>，用于从互联网爬取数据。</p>
<p><strong>本章内容概览</strong></p>
<ul>
<li><strong>Python 生态工具</strong>：
<ul>
<li>如何管理多个 Python 版本</li>
<li>创建虚拟环境</li>
<li>安装固定版本的依赖项</li>
<li>如何在本地安装 <code>LLM-Engineers-Handbook</code> 代码库（如果你想尝试代码，地址如下）：<a target="_blank" rel="noopener" href="https://github.com/PacktPublishing/LLM-Engineers-Handbook">GitHub Repo</a></li>
</ul>
</li>
<li><strong>MLOps &amp; LLMOps 工具链</strong>：
<ul>
<li>介绍<strong>通用 MLOps 工具</strong>（如模型注册表）</li>
<li>深入了解<strong>LLM 相关工具</strong>（如 LLM 评估和 Prompt 监控工具）</li>
<li>使用 <strong>ZenML</strong> 进行 ML 管道管理（ML 与 MLOps 的桥梁）</li>
</ul>
</li>
<li><strong>数据库管理</strong>：
<ul>
<li>介绍 <strong>NoSQL 和向量数据库</strong> 的使用</li>
<li>如何使用 <strong>Docker</strong> 在本地运行所有组件</li>
</ul>
</li>
<li><strong>云端环境准备（AWS）</strong>：
<ul>
<li>创建 AWS 账户并获取访问密钥</li>
<li>安装 &amp; 配置 <strong>AWS CLI</strong>，以便程序化管理云资源</li>
<li>了解 <strong>SageMaker</strong> 及其在开源 LLM 训练和部署中的作用</li>
</ul>
</li>
</ul>
<h2 id="python-生态工具链安装"><a class="markdownIt-Anchor" href="#python-生态工具链安装"></a> Python 生态工具链安装</h2>
<p>任何 Python 项目都需要三个基本工具：<strong><code>python</code> 解释器</strong>、<strong>依赖项管理</strong>和<strong>任务执行工具</strong>。Python 解释器会按预期执行您的 Python 项目。手册中的所有代码都使用 <code>python 3.11.8</code> 进行了测试。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python --version</span><br><span class="line"><span class="comment"># Python 3.11.8</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>[!TIP]</p>
<p>我们可以从此处下载 <code>python 解释器</code>：<a target="_blank" rel="noopener" href="https://www.python.org/downloads/"><code>python.org</code></a></p>
</blockquote>
<p>手册中推荐使用 <strong><code>poetry</code> 工具</strong>来管理 <code>python</code>的依赖和虚拟环境，<code>poethepoet</code> 是 <code>Poetry</code> 插件，用于<strong>管理 CLI 任务</strong>，替代 <code>Makefile</code>、<code>shell</code> 脚本等；当然也可以使用其他的工具的组合替代，比如 <code>conda(mini-conda)</code> 和 <code>pip</code>。</p>
<ul>
<li><code>poetry</code> 官网：<a target="_blank" rel="noopener" href="https://python-poetry.org/docs/">https://python-poetry.org/docs/</a></li>
<li><code>mini-conda</code> 官网：<a target="_blank" rel="noopener" href="https://www.anaconda.com/docs/getting-started/miniconda/main">https://www.anaconda.com/docs/getting-started/miniconda/main</a></li>
</ul>
<h2 id="mlops-llmops-工具概览"><a class="markdownIt-Anchor" href="#mlops-llmops-工具概览"></a> MLOps &amp; LLMOps 工具概览</h2>
<p>本节简要介绍 <strong>MLOps</strong>（机器学习运维）和 <strong>LLMOps</strong>（大模型运维）工具，以及它们在 <code>LLM Twin</code> 项目中的作用。理论部分将在第 11 章深入讲解，而手册主要通过<strong>实战</strong>来展示这些工具的使用方式。</p>
<h3 id="hugging-face模型注册库model-registry"><a class="markdownIt-Anchor" href="#hugging-face模型注册库model-registry"></a> Hugging Face：模型注册库（Model Registry）</h3>
<p><strong>模型注册库</strong> 是一个 <strong>集中式存储</strong>，用于管理 <strong>ML 模型</strong> 的<strong>版本、元数据和性能指标</strong>。它在 MLOps 中起到关键作用：</p>
<ul>
<li>版本控制（Versioning）</li>
<li>模型共享（Sharing）</li>
<li>模型可追溯性（Traceability）</li>
<li>集成 CI/CD 流水线（Continuous Deployment）</li>
</ul>
<p>以下是一些常见的模型库：</p>
<ul>
<li><strong>Hugging Face Model Hub</strong>
<ul>
<li>社区丰富，但中国大陆境内无法直接访问 ❌</li>
<li><strong>适用于</strong>：开源社区、NLP、大模型（LLMs）</li>
<li><strong>官网</strong>：<a target="_blank" rel="noopener" href="https://huggingface.co">https://huggingface.co</a></li>
<li><strong>特点</strong>：
<ol>
<li><strong>大模型（LLM）生态支持</strong>，可存储 Transformer、Diffusion 等 AI 模型。</li>
<li>提供 <strong>可视化 UI</strong>，可管理模型版本、推理 demo（Spaces）。</li>
<li>易于与 <strong>PyTorch、TensorFlow、JAX</strong> 等框架集成。</li>
<li>适用于 <strong>团队协作</strong> 和 <strong>社区共享</strong>，支持 <strong>私有模型库</strong>。</li>
</ol>
</li>
</ul>
</li>
<li><strong>AWS SageMaker Model Registry</strong>
<ul>
<li><strong>适用于：</strong> 企业级云端 MLOps，<strong>企业、金融、医疗</strong>等对安全性要求高的 MLOps 场景。</li>
<li><strong>官网：</strong> <a target="_blank" rel="noopener" href="https://aws.amazon.com/sagemaker/">aws.amazon.com/sagemaker/</a></li>
<li><strong>特点：</strong>
<ol>
<li>与 <strong>AWS SageMaker</strong> 生态完美结合，支持 <strong>训练-注册-部署-监控</strong> 整套流程。</li>
<li>提供 <strong>自动化 CI/CD</strong>，模型更新后可自动部署。</li>
<li><strong>高安全性</strong>，支持 <strong>IAM 权限管理</strong>，适用于企业级 ML 部署。</li>
</ol>
</li>
</ul>
</li>
<li><strong>ModelScope</strong>
<ul>
<li>阿里云推出的开源 AI 模型平台</li>
<li><strong>适用于</strong>：适用于 <strong>大模型（LLM）、计算机视觉（CV）、自然语言处理（NLP）</strong> 等 AI 任务</li>
<li><strong>官网</strong>：<a target="_blank" rel="noopener" href="https://modelscope.cn/">https://modelscope.cn/</a></li>
<li><strong>特点</strong>：
<ol>
<li>支持 1,000+ 预训练模型，包括大语言模型（LLM）、计算机视觉（CV）、语音处理（Speech）、多模态（Multimodal）等</li>
<li>一键调用 AI 工作流（Pipelines），快速搭建 AI 任务</li>
<li>免费在线体验，支持<strong>模型下载、本地部署、API 调用</strong></li>
</ol>
</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th><strong>对比项</strong></th>
<th><strong>魔搭 ModelScope</strong></th>
<th><strong>Hugging Face Model Hub</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>适用地区</strong></td>
<td>✅ <strong>国内可用</strong></td>
<td>🚫 <strong>需科学上网</strong></td>
</tr>
<tr>
<td><strong>模型数量</strong></td>
<td>⭐ 1,000+</td>
<td>⭐ 10,000+</td>
</tr>
<tr>
<td><strong>大模型支持</strong></td>
<td>✅ 通义千问、ChatGLM、Qwen</td>
<td>✅ LLaMA、GPT-3、Falcon</td>
</tr>
<tr>
<td><strong>微调（Fine-tuning）</strong></td>
<td>✅ LoRA, QLoRA, P-Tuning</td>
<td>✅ LoRA, PEFT, DPO, Unsloth</td>
</tr>
<tr>
<td><strong>推理服务（API）</strong></td>
<td>✅ 免费调用</td>
<td>✅ 需付费</td>
</tr>
<tr>
<td><strong>工作流（Pipelines）</strong></td>
<td>✅ 一键执行</td>
<td>🚫 需手写代码，可与 <code>LLMOps</code> 集成</td>
</tr>
<tr>
<td><strong>私有化部署</strong></td>
<td>✅ 企业可自建</td>
<td>✅ 需自建服务器</td>
</tr>
</tbody>
</table>
<blockquote>
<p>[!NOTE]</p>
<p>手册中使用的 Hugging Face 模型：</p>
<ul>
<li><code>TwinLlama 3.1-8B</code>（微调后）<a target="_blank" rel="noopener" href="https://huggingface.co/mlabonne/TwinLlama-3.1-8B">https://huggingface.co/mlabonne/TwinLlama-3.1-8B</a></li>
<li><code>TwinLlama 3.1-8B-DPO</code>（偏好对齐后）<a target="_blank" rel="noopener" href="https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO">https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO</a></li>
</ul>
</blockquote>
<h3 id="zenmlmlops-工作流编排器"><a class="markdownIt-Anchor" href="#zenmlmlops-工作流编排器"></a> ZenML：MLOps 工作流编排器</h3>
<blockquote>
<p>[!NOTE]</p>
<p>TODO</p>
</blockquote>
<h3 id="cometml可视化实验跟踪"><a class="markdownIt-Anchor" href="#cometml可视化实验跟踪"></a> CometML：可视化实验跟踪</h3>
<blockquote>
<p>[!NOTE]</p>
<p>TODO</p>
</blockquote>
<h3 id="opik评估-测试和监控大型语言模型"><a class="markdownIt-Anchor" href="#opik评估-测试和监控大型语言模型"></a> Opik：评估、测试和监控大型语言模型</h3>
<blockquote>
<p>[!NOTE]</p>
<p>TODO</p>
</blockquote>
<h2 id="非结构化数据库与向量数据库"><a class="markdownIt-Anchor" href="#非结构化数据库与向量数据库"></a> 非结构化数据库与向量数据库</h2>
<h3 id="mongodbnosql"><a class="markdownIt-Anchor" href="#mongodbnosql"></a> MongoDB：NoSQL</h3>
<blockquote>
<p>[!NOTE]</p>
<p>TODO</p>
</blockquote>
<h3 id="qdrant向量数据库"><a class="markdownIt-Anchor" href="#qdrant向量数据库"></a> Qdrant：向量数据库</h3>
<blockquote>
<p>[!NOTE]</p>
<p>TODO</p>
</blockquote>
<h2 id="云端环境准备aws"><a class="markdownIt-Anchor" href="#云端环境准备aws"></a> 云端环境准备（AWS）</h2>
<blockquote>
<p>[!NOTE]</p>
<p>TODO</p>
</blockquote>
<h1 id="三-数据收集"><a class="markdownIt-Anchor" href="#三-数据收集"></a> 三、数据收集</h1>
<p>本章将深入探讨 <code>LLM Twin</code> 项目，学习如何设计和实现数据收集流水线，以获取用于 LLM 任务（如微调或推理）的原始数据。由于本书并非专门介绍数据工程，因此本章内容将保持精简，仅关注收集必要原始数据的关键部分。从第 4 章开始，我们将重点讨论 <strong>LLM 和生成式 AI</strong>，深入研究其理论和具体实现细节。</p>
<p>在处理项目或研究时，我们通常会使用一个<strong>静态数据集</strong>。但在 <code>LLM Twin</code> 项目中，我们希望模拟真实世界的场景，在其中<strong>主动收集和整理数据</strong>。因此，构建<strong>数据流水线</strong>将帮助我们了解端到端机器学习项目的工作方式。本章将讲解如何设计和实现 <strong>ETL（提取、转换、加载）流水线</strong>，从 社交平台爬取数据，并将其存储到 <strong>MongoDB 数据库</strong>。我们将介绍各种爬取方法，标准化数据，并将其加载到数据仓库中。</p>
<p>本章的主要内容包括：</p>
<ol>
<li><strong>设计数据收集流水线</strong>
<ul>
<li>介绍 <strong>LLM Twin</strong> 的数据收集架构</li>
<li>解析 <strong>ETL</strong> 流水线的设计</li>
</ul>
</li>
<li><strong>实现数据收集流水线</strong>
<ul>
<li>使用 <strong>ZenML</strong> 作为流程编排工具</li>
<li>构建爬虫，并实现 <strong>调度层</strong>（根据 URL 域名实例化对应爬虫类）</li>
<li>按最佳软件开发实践，开发每个爬虫模块</li>
</ul>
</li>
<li><strong>数据仓库管理</strong>
<ul>
<li>在 <strong>MongoDB</strong> 之上构建数据层，统一管理文档结构</li>
<li>查询并交互数据</li>
</ul>
</li>
</ol>
<p>最后，我们将学习如何使用 <strong>ZenML 运行数据收集流水线</strong>，并<strong>查询 MongoDB 中的数据</strong>。</p>
<h2 id="设计数据收集管道"><a class="markdownIt-Anchor" href="#设计数据收集管道"></a> 设计数据收集管道</h2>
<p>在深入实施之前，我们必须了解 <code>LLM Twin</code> 的数据收集 ETL 架构，如下图所示。我们需要探究从哪些平台抓取数据，以及如何设计数据结构和流程。但是，第一步是了解我们的数据收集管道如何映射到 ETL 流程。</p>
<p><img src="/2025/03/10/llm-engineers-handbook-note/screenshot_2025-03-11_15.25.00.png" alt="screenshot_2025-03-11_15.25.00"></p>
<p>ETL 管道涉及三个基本步骤：</p>
<ol>
<li>我们从各种来源<strong>提取数据</strong>。我们将从内容平台抓取数据以收集原始数据。</li>
<li>我们通过<strong>清理和标准化这些数据</strong>，将其转换为适合存储和分析的一致格式。</li>
<li>我们将转换后的数据<strong>加载到数据仓库</strong>或数据库中。</li>
</ol>
<p>对于我们的项目，我们使用 MongoDB 作为我们的 NoSQL 数据仓库。虽然这不是标准方法，但我们很快就会解释这种选择背后的原因。<br>
我们想要设计一个 ETL 管道，输入一个用户和一个链接列表作为输入。之后，它会单独抓取每个链接，标准化收集到的内容，并将其保存在 MongoDB 数据仓库中该特定作者下。</p>
<p><strong>数据收集流水线的输入和输出</strong></p>
<ul>
<li><strong>输入</strong>：用户（作者）及其提供的一组链接。</li>
<li><strong>输出</strong>：存储在 <strong>MongoDB</strong> 数据仓库中的原始文档列表。</li>
</ul>
<blockquote>
<p>[!NOTE]</p>
<p>我们将交替使用<strong>用户</strong>和<strong>作者</strong>，因为在 ETL 管道的大多数情况下，用户是提取内容的作者。但是，在数据仓库中，我们只有一个用户集合。</p>
</blockquote>
<p><img src="/2025/03/10/llm-engineers-handbook-note/screenshot_2025-03-11_15.38.12.png" alt="screenshot_2025-03-11_15.38.12">ETL 管道将检测每个链接的域名，并根据该域调用专门的爬虫。我们为三个不同的数据类别实现了四个不同的爬虫，如上图所示。首先，我们收集的所有文档都可以归结为文章、存储库（或代码)和帖子。数据来自哪里并不重要。我们主要对文档的格式感兴趣。在大多数情况下，我们必须以不同的方式处理这些数据类别。因此，我们为每个实体创建了一个不同的域实体，每个实体在 MongoDB 中都有自己的类和集合。当我们将源 URL 保存在文档的元数据中时，我们仍然会知道它的来源，并可以在 GenAI 用例中引用它。</p>
<table>
<thead>
<tr>
<th>爬虫类型</th>
<th>目标数据源</th>
<th>输出文档类型</th>
<th>主要步骤</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Medium 爬虫</strong></td>
<td>Medium 文章</td>
<td>文章（Article）</td>
<td>登录 Medium → 爬取 HTML → 解析并清理文本 → 存入数据库</td>
</tr>
<tr>
<td><strong>通用文章爬虫</strong></td>
<td>Substack / 个人博客等</td>
<td>文章（Article）</td>
<td>爬取 HTML → 解析并清理文本 → 存入数据库</td>
</tr>
<tr>
<td><strong>GitHub 爬虫</strong></td>
<td>GitHub 仓库</td>
<td>代码仓库（Repository）</td>
<td>克隆代码仓库 → 解析文件树 → 处理代码文件 → 存入数据库</td>
</tr>
<tr>
<td><strong>LinkedIn 爬虫</strong></td>
<td>LinkedIn 个人动态</td>
<td>帖子（Post）</td>
<td>登录 LinkedIn → 爬取用户动态 → 解析 HTML → 存入数据库</td>
</tr>
</tbody>
</table>
<p>在下一节中，我们将详细研究每个爬虫的实现。现在，请注意，每个爬虫都以特定方式访问特定平台或站点并从中提取 HTML。之后，所有爬虫都会解析 HTML，从中提取文本，并对其进行清理和规范化，以便可以将其存储在同一个接口下的数据仓库中。</p>
<p>通过将所有收集的数据减少到三种数据类别，<strong>文章（Article）</strong>、<strong>代码仓库（Repository）<strong>和</strong>帖子（Post）</strong>，而不是为每个新数据源创建新的数据类别，我们可以轻松地将此架构扩展到多个数据源，而无需付出太多重复适配工作。例如，如果我们想开始从 X 收集数据，我们只需要实现一个输出帖子文档的新爬虫，仅此而已。其余代码将保持不变。否则，如果我们在类和文档结构中引入源维度，我们将不得不向所有下游层添加代码以支持任何新数据源。例如，我们必须为每个新源实现一个新的文档类，并调整功能管道以支持它。</p>
<p>对于我们的概念验证，抓取几百个文档就足够了，但如果我们想将其扩展到实际产品，我们可能需要更多数据源来抓取。LLM 需要大量数据。因此，您需要数千个文档才能获得理想的结果，而不仅仅是几百个文档。但在许多项目中，实现一个不是最准确的端到端项目版本并在以后对其进行迭代是一种很好的策略。因此，通过使用这种架构，可以在未来的迭代中轻松添加更多数据源以收集更大的数据集。下一章将介绍有关 LLM 微调和数据集大小的更多信息。</p>
<p>ETL 过程如何连接到特征管道？特征管道从 MongoDB 数据仓库中提取原始数据，进一步清理，将其处理为特征，并将其存储在 Qdrant 向量数据库中，以使 LLM 训练和推理管道可以访问它。第 4 章提供了有关特征管道的更多信息。ETL 过程独立于特征管道。这两个管道严格通过 MongoDB 数据仓库相互通信。因此，数据收集管道可以为 MongoDB 写入数据，而功能管道可以独立地按照不同的时间表从中读取数据。</p>
<p><strong>为什么我们使用 MongoDB 作为数据仓库？</strong></p>
<ul>
<li><strong>适用于小规模数据</strong>：本项目的文档量较小，MongoDB 能够很好地处理。</li>
<li><strong>适合非结构化文本</strong>：爬取的数据主要是<strong>非结构化文本</strong>，MongoDB 不强制模式（Schema），使开发更灵活。</li>
<li><strong>易用性</strong>：MongoDB 提供直观的 <strong>Python SDK</strong>，官方提供 <strong>Docker 镜像</strong> 和 <strong>云端免费层</strong>，适合本项目的 PoC（概念验证）。</li>
<li><strong>未来可扩展性</strong>：如果数据量增大（如达到百万级别），可以切换到 <strong>Snowflake</strong> 或 <strong>BigQuery</strong> 这样的专用数据仓库。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://lostnfound.top">Guohao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://lostnfound.top/2025/03/10/llm-engineers-handbook-note/">https://lostnfound.top/2025/03/10/llm-engineers-handbook-note/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://lostnfound.top" target="_blank">Lost N Found</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/DeepLearning/">DeepLearning</a><a class="post-meta__tags" href="/tags/Engineering/">Engineering</a></div><div class="post_share"><div class="social-share" data-image="/linear-gradient(45deg,%20#8EC3B0,%20#9ED5C5,%20#F8C4B4,%20#FF8787)" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Guohao</div><div class="author-info__description">L’existence précède l‘essence</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">42</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Dave0126" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:dave980126@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9B%B6-%E5%89%8D%E8%A8%80"><span class="toc-text"> 零、前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80-%E7%90%86%E8%A7%A3-llm-twin-%E7%9A%84%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9E%B6%E6%9E%84"><span class="toc-text"> 一、理解 LLM Twin 的概念和架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-llm-twin"><span class="toc-text"> 什么是 LLM Twin</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%94%A8-qwen-%E8%BF%99%E4%BA%9B%E9%80%9A%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-text"> 为什么不用 Qwen 这些通用大模型？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%84%E5%88%92%E4%BA%A7%E5%93%81%E7%9A%84-mvp"><span class="toc-text"> 规划产品的 MVP</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-mvp"><span class="toc-text"> 什么是 MVP?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#llm-twin-%E7%9A%84-mvp-%E7%9A%84%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD"><span class="toc-text"> LLM Twin 的 MVP 的核心功能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mvp-%E7%9A%84%E5%85%B3%E9%94%AE%E6%8C%91%E6%88%98"><span class="toc-text"> MVP 的关键挑战</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E5%85%B7%E6%9C%89%E7%89%B9%E5%BE%81%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86%E6%B5%81%E6%B0%B4%E7%BA%BF%E7%9A%84-ml-%E7%B3%BB%E7%BB%9F"><span class="toc-text"> 构建具有特征&#x2F;训练&#x2F;推理流水线的 ML 系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ml-%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E7%9A%84%E6%8C%91%E6%88%98"><span class="toc-text"> ML 系统开发的挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%BB%9F%E4%B8%80%E7%9A%84-ml-%E7%B3%BB%E7%BB%9F"><span class="toc-text"> 如何构建一个统一的 ML 系统？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A5%E5%BE%80%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text"> 以往解决方案的问题</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8D%95%E4%BD%93%E6%9E%B6%E6%9E%84%E5%9C%A8%E5%AE%9E%E6%97%B6%E6%8E%A8%E7%90%86%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text"> 单体架构在实时推理系统中的问题</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86fti-%E6%9E%B6%E6%9E%84"><span class="toc-text"> 特征&#x2F;训练&#x2F;推理（FTI） 架构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E7%AE%A1%E9%81%93feature-pipeline"><span class="toc-text"> 特征管道（Feature Pipeline）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%AE%A1%E9%81%93training-pipeline"><span class="toc-text"> 训练管道（Training Pipeline）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E7%AE%A1%E9%81%93inference-pipeline"><span class="toc-text"> 推理管道（Inference Pipeline）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1-llm-twin-%E7%9A%84%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84"><span class="toc-text"> 设计 LLM Twin 的系统结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90"><span class="toc-text"> 需求分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83training%E9%9C%80%E6%B1%82"><span class="toc-text"> 训练（Training）需求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A8%E7%90%86inference%E9%9C%80%E6%B1%82"><span class="toc-text"> 推理（Inference）需求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#llmops-%E9%9C%80%E6%B1%82"><span class="toc-text"> LLMOps 需求</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-fti-%E7%AE%A1%E9%81%93%E8%AE%BE%E8%AE%A1-llm-twin-%E6%9E%B6%E6%9E%84"><span class="toc-text"> 如何使用 FTI 管道设计 LLM Twin 架构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E7%AE%A1%E9%81%93data-collection-pipeline"><span class="toc-text"> 数据收集管道（Data Collection Pipeline）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E7%AE%A1%E9%81%93feature-pipeline-2"><span class="toc-text"> 特征管道（Feature Pipeline）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%AE%A1%E9%81%93training-pipeline-2"><span class="toc-text"> 训练管道（Training Pipeline）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E7%AE%A1%E9%81%93inference-pipeline-2"><span class="toc-text"> 推理管道（Inference Pipeline）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fti-%E8%AE%BE%E8%AE%A1%E4%B8%8E-llm-twin-%E6%9E%B6%E6%9E%84%E7%9A%84%E6%9C%80%E7%BB%88%E6%80%9D%E8%80%83"><span class="toc-text"> FTI 设计与 LLM Twin 架构的最终思考</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E7%94%A8"><span class="toc-text"> 引用</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C-%E5%B7%A5%E5%85%B7%E9%93%BE%E4%B8%8E%E5%AE%89%E8%A3%85"><span class="toc-text"> 二、工具链与安装</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#python-%E7%94%9F%E6%80%81%E5%B7%A5%E5%85%B7%E9%93%BE%E5%AE%89%E8%A3%85"><span class="toc-text"> Python 生态工具链安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mlops-llmops-%E5%B7%A5%E5%85%B7%E6%A6%82%E8%A7%88"><span class="toc-text"> MLOps &amp; LLMOps 工具概览</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#hugging-face%E6%A8%A1%E5%9E%8B%E6%B3%A8%E5%86%8C%E5%BA%93model-registry"><span class="toc-text"> Hugging Face：模型注册库（Model Registry）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zenmlmlops-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%BC%96%E6%8E%92%E5%99%A8"><span class="toc-text"> ZenML：MLOps 工作流编排器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cometml%E5%8F%AF%E8%A7%86%E5%8C%96%E5%AE%9E%E9%AA%8C%E8%B7%9F%E8%B8%AA"><span class="toc-text"> CometML：可视化实验跟踪</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#opik%E8%AF%84%E4%BC%B0-%E6%B5%8B%E8%AF%95%E5%92%8C%E7%9B%91%E6%8E%A7%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-text"> Opik：评估、测试和监控大型语言模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%9E%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8E%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text"> 非结构化数据库与向量数据库</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#mongodbnosql"><span class="toc-text"> MongoDB：NoSQL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#qdrant%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text"> Qdrant：向量数据库</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%91%E7%AB%AF%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87aws"><span class="toc-text"> 云端环境准备（AWS）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89-%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86"><span class="toc-text"> 三、数据收集</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E7%AE%A1%E9%81%93"><span class="toc-text"> 设计数据收集管道</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: #8EC3B0"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2025 By Guohao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: 'ea138c6f176d57705144',
      clientSecret: 'c999d74b366c68c80bc3b704c716a8ff8d67af6d',
      repo: 'Dave0126.github.io',
      owner: 'Dave0126',
      admin: ['Dave0126'],
      id: '07a89b9dbc2537b90e6f494d6bb4dcbf',
      updateCountCallback: commentCount
    },))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div></div></body></html>